{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dce670",
   "metadata": {},
   "source": [
    "This notebook is supposed to be created to perform tests on the MGA-YOLOv8 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73847e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mariopasc/Python/Datasets/COMBINED/YOLO_MGA/detection/images/val/arcadetest_p66_v66_00066.png: 640x640 1 person, 5.8ms\n",
      "Speed: 0.9ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    \"\"\"Channel Attention Module.\n",
    "\n",
    "    This module generates a channel attention map by exploiting both max-pooled\n",
    "    and average-pooled features along the spatial dimensions.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Number of input channels.\n",
    "        r (int): Reduction ratio for the MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, r: int) -> None:\n",
    "        super(CAM, self).__init__()\n",
    "        if channels <= 0 or r <= 0 or channels % r != 0:\n",
    "            raise ValueError(\n",
    "                f\"Invalid parameters: channels={channels}, r={r}. \"\n",
    "                f\"Ensure channels > 0, r > 0, and channels is divisible by r.\"\n",
    "            )\n",
    "\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=self.channels,\n",
    "                out_features=self.channels // self.r,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(\n",
    "                in_features=self.channels // self.r,\n",
    "                out_features=self.channels,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply channel attention mechanism.\"\"\"\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "\n",
    "        # Global max pooling\n",
    "        max_pool = torch.nn.functional.adaptive_max_pool2d(x, output_size=1).view(batch_size, channels)\n",
    "        # Global average pooling\n",
    "        avg_pool = torch.nn.functional.adaptive_avg_pool2d(x, output_size=1).view(batch_size, channels)\n",
    "\n",
    "        # Apply shared MLP to both pooled features\n",
    "        max_out = self.mlp(max_pool).view(batch_size, channels, 1, 1)\n",
    "        avg_out = self.mlp(avg_pool).view(batch_size, channels, 1, 1)\n",
    "\n",
    "        # Combine and create attention map\n",
    "        attention = torch.sigmoid(max_out + avg_out)\n",
    "\n",
    "        return attention * x\n",
    "\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    \"\"\"Spatial Attention Module.\n",
    "\n",
    "    This module generates a spatial attention map by utilizing max-pooled and\n",
    "    average-pooled features along the channel dimension.\n",
    "\n",
    "    Args:\n",
    "        bias (bool, optional): Whether to include bias in the convolution layer. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bias: bool = False) -> None:\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=1,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=3,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply spatial attention mechanism.\"\"\"\n",
    "        # Max pooling along channel dimension\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        # Average pooling along channel dimension\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "\n",
    "        # Concatenate pooled features\n",
    "        concat = torch.cat((max_pool, avg_pool), dim=1)\n",
    "\n",
    "        # Generate spatial attention map\n",
    "        spatial_map = torch.sigmoid(self.conv(concat))\n",
    "\n",
    "        return spatial_map * x\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module (CBAM).\n",
    "\n",
    "    This module combines channel attention and spatial attention mechanisms to\n",
    "    enhance feature representation by focusing on 'what' and 'where' information.\n",
    "\n",
    "    Reference:\n",
    "        \"CBAM: Convolutional Block Attention Module\"\n",
    "        https://arxiv.org/abs/1807.06521\n",
    "\n",
    "    Args:\n",
    "        channels (int): Number of input channels.\n",
    "        reduction_ratio (int): Reduction ratio for the channel attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, r: int) -> None:\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.reduction_ratio = r\n",
    "        self.channel_attention = CAM(channels=self.channels, r=self.reduction_ratio)\n",
    "        self.spatial_attention = SAM(bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply channel and spatial attention sequentially.\"\"\"\n",
    "        cam_output = self.channel_attention(x)\n",
    "        cam_output = cam_output * x\n",
    "        sam_output = self.spatial_attention(cam_output)\n",
    "        x_refined = cam_output * sam_output\n",
    "\n",
    "        # Skipped connection\n",
    "        if x.shape != x_refined.shape:\n",
    "            raise ValueError(\n",
    "                f\"Input shape {x.shape} does not match refined shape {x_refined.shape}.\"\n",
    "            )\n",
    "        # Apply skip connection\n",
    "        x_refined = x + x*x_refined\n",
    "        return x_refined\n",
    "\n",
    "\n",
    "class CBAMMaskedFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A hook manager that extracts feature maps from specified layers,\n",
    "    applies corresponding masks, and enhances them with CBAM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_layers: List[str], mask_folder: str, alpha: float = 0.5, reduction_ratio: int = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor.\n",
    "\n",
    "        Args:\n",
    "            target_layers: List of layer names to extract features from\n",
    "            mask_folder: Path to the folder containing image masks\n",
    "            alpha: Weight for skip connection (0.0 to 1.0)\n",
    "                   feature_map = (1-alpha)*feature_map + alpha*(CBAM(feature_map*mask))\n",
    "            reduction_ratio: Reduction ratio for CBAM module\n",
    "        \"\"\"\n",
    "        self.target_layers = target_layers\n",
    "        self.mask_folder = mask_folder\n",
    "        self.alpha = max(0.0, min(1.0, alpha))  # Clamp between 0 and 1\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Storage dictionaries\n",
    "        self.feature_maps: Dict[str, torch.Tensor] = {}\n",
    "        self.masked_feature_maps: Dict[str, torch.Tensor] = {}\n",
    "        self.current_image_path: Optional[str] = None\n",
    "        self.image_names: Dict[str, str] = {}\n",
    "        \n",
    "        # CBAM modules for each layer (will be created dynamically)\n",
    "        self.cbam_modules: Dict[str, CBAM] = {}\n",
    "        \n",
    "    def set_current_image(self, image_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the current image being processed.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "        \"\"\"\n",
    "        self.current_image_path = image_path\n",
    "        \n",
    "    def register_hooks(self, model: nn.Module) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Register hooks to extract feature maps from the model.\n",
    "\n",
    "        Args:\n",
    "            model: The YOLO model to attach hooks to\n",
    "\n",
    "        Returns:\n",
    "            The model with hooks attached\n",
    "        \"\"\"\n",
    "        # Clear existing hooks and storage\n",
    "        self.clear_hooks()\n",
    "        self.feature_maps = {}\n",
    "        self.masked_feature_maps = {}\n",
    "        self.image_names = {}\n",
    "        self.cbam_modules = {}\n",
    "\n",
    "        # Register hooks on target layers\n",
    "        if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
    "            for name, module in model.model.named_modules():\n",
    "                if name in self.target_layers:\n",
    "                    # Register hook to capture feature map\n",
    "                    hook = self._create_feature_hook(name)\n",
    "                    self.hooks.append(module.register_forward_hook(hook))\n",
    "                    \n",
    "        return model\n",
    "\n",
    "    def _create_feature_hook(self, layer_name: str):\n",
    "        \"\"\"\n",
    "        Create a hook function to store feature maps and apply masks with CBAM.\n",
    "\n",
    "        Args:\n",
    "            layer_name: Name of the layer for reference\n",
    "\n",
    "        Returns:\n",
    "            Hook function to be registered\n",
    "        \"\"\"\n",
    "        def hook(module, input_feat, output):\n",
    "            # Store original feature map\n",
    "            self.feature_maps[layer_name] = output.clone()\n",
    "            \n",
    "            # Store the image name if available\n",
    "            if self.current_image_path:\n",
    "                image_name = Path(self.current_image_path).stem\n",
    "                self.image_names[layer_name] = image_name\n",
    "                \n",
    "                # Find and apply mask if available\n",
    "                mask_path = self._find_mask_path(image_name)\n",
    "                if mask_path:\n",
    "                    # Process the mask and apply it\n",
    "                    mask_tensor = self._process_mask(mask_path, output.shape[2:])\n",
    "                    if mask_tensor is not None:\n",
    "                        mask_tensor = mask_tensor.to(output.device)\n",
    "                        masked_output = self._apply_mask_with_cbam(output, mask_tensor, layer_name)\n",
    "                        self.masked_feature_maps[layer_name] = masked_output\n",
    "            \n",
    "            # Don't modify the output passing through the model\n",
    "            return output\n",
    "            \n",
    "        return hook\n",
    "    \n",
    "    def _find_mask_path(self, image_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find the corresponding mask file for an image.\n",
    "        \n",
    "        Args:\n",
    "            image_name: Base name of the image without extension\n",
    "            \n",
    "        Returns:\n",
    "            Path to the mask file if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Try the exact same filename with different possible extensions\n",
    "        for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "            mask_path = os.path.join(self.mask_folder, f\"{image_name}{ext}\")\n",
    "            if os.path.exists(mask_path):\n",
    "                return mask_path\n",
    "        \n",
    "        # If not found, look for any file starting with the image name\n",
    "        if os.path.exists(self.mask_folder):\n",
    "            for filename in os.listdir(self.mask_folder):\n",
    "                if filename.startswith(image_name):\n",
    "                    return os.path.join(self.mask_folder, filename)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _process_mask(self, mask_path: str, target_size: Tuple[int, int]) -> Optional[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load and process a mask to match the feature map dimensions.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to the mask file\n",
    "            target_size: Target size as (height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Processed mask tensor or None if processing failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load mask as grayscale image\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            \n",
    "            # Resize to match feature map dimensions\n",
    "            resized_mask = transforms.Resize(\n",
    "                target_size, interpolation=transforms.InterpolationMode.NEAREST\n",
    "            )(mask)\n",
    "            \n",
    "            # Convert to tensor [1, 1, H, W]\n",
    "            mask_tensor = transforms.ToTensor()(resized_mask).unsqueeze(0)\n",
    "            \n",
    "            return mask_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing mask {mask_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _apply_mask_with_cbam(self, feature_map: torch.Tensor, mask: torch.Tensor, layer_name: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply mask to feature map, enhance with CBAM, and combine with skip connection.\n",
    "        \n",
    "        Implements: feature_map = (1-alpha)*feature_map + alpha*(CBAM(feature_map*mask))\n",
    "        \n",
    "        Args:\n",
    "            feature_map: Input feature map [B, C, H, W]\n",
    "            mask: Binary mask [1, 1, H, W]\n",
    "            layer_name: Name of the layer for CBAM module caching\n",
    "            \n",
    "        Returns:\n",
    "            Modified feature map with same shape as input\n",
    "        \"\"\"\n",
    "        # Expand mask to match feature map channels\n",
    "        B, C, H, W = feature_map.shape\n",
    "        expanded_mask = mask.expand(B, C, H, W)\n",
    "        \n",
    "        # Apply mask to get masked feature\n",
    "        masked_feature = feature_map * expanded_mask\n",
    "        \n",
    "        # Create or reuse CBAM module for this layer\n",
    "        if layer_name not in self.cbam_modules:\n",
    "            self.cbam_modules[layer_name] = CBAM(\n",
    "                channels=C, \n",
    "                r=self.reduction_ratio\n",
    "            ).to(feature_map.device)\n",
    "        \n",
    "        # Apply CBAM to the masked feature\n",
    "        enhanced_masked_feature = self.cbam_modules[layer_name](masked_feature)\n",
    "        \n",
    "        # Apply skip connection\n",
    "        output = (1 - self.alpha) * feature_map + self.alpha * enhanced_masked_feature\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def clear_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks to prevent memory leaks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def get_feature_maps(self, include_masked: bool = True) -> Dict[str, Dict[str, Union[torch.Tensor, str]]]:\n",
    "        \"\"\"\n",
    "        Get the captured feature maps with their associated image names.\n",
    "        \n",
    "        Args:\n",
    "            include_masked: Whether to include masked versions of feature maps\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with layer names as keys and dict of data as values\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for layer_name, feature_map in self.feature_maps.items():\n",
    "            layer_data = {\n",
    "                'original': feature_map,\n",
    "                'image_name': self.image_names.get(layer_name)\n",
    "            }\n",
    "            \n",
    "            if include_masked and layer_name in self.masked_feature_maps:\n",
    "                layer_data['masked'] = self.masked_feature_maps[layer_name]\n",
    "                \n",
    "            result[layer_name] = layer_data\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def set_alpha(self, alpha: float) -> None:\n",
    "        \"\"\"\n",
    "        Set the weight for the skip connection.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Value between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        self.alpha = max(0.0, min(1.0, alpha))\n",
    "\n",
    "# Example usage in notebook\n",
    "\n",
    "from ICA_Detection.external.ultralytics.ultralytics import YOLO\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base = \"/home/mariopasc/Python/Datasets/COMBINED/YOLO_MGA\"\n",
    "\n",
    "# Image and mask paths\n",
    "image_path = os.path.join(base, \"detection\", \"images\", \"val\", \"arcadetest_p66_v66_00066.png\")\n",
    "mask_folder = os.path.join(base, \"masks\")\n",
    "test_path = \"/home/mariopasc/Python/Datasets/COMBINED/detection/testing\"\n",
    "\n",
    "# Initialize the feature extractor with target layers and mask folder\n",
    "target_layers = [\"model.15\", \"model.18\", \"model.21\"]\n",
    "feature_extractor = CBAMMaskedFeatureExtractor(\n",
    "    target_layers=target_layers,\n",
    "    mask_folder=mask_folder,\n",
    "    alpha=0.7,  # Weight for mask application (adjust as needed)\n",
    "    reduction_ratio=16  # Reduction ratio for CBAM\n",
    ")\n",
    "\n",
    "# Load your YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Register hooks\n",
    "feature_extractor.register_hooks(model)\n",
    "\n",
    "# Set the current image path before inference\n",
    "feature_extractor.set_current_image(image_path)\n",
    "\n",
    "# Run inference with your model\n",
    "results = model(image_path)\n",
    "\n",
    "# Get the extracted feature maps with image names\n",
    "feature_maps_data = feature_extractor.get_feature_maps()\n",
    "\n",
    "# Visualize the original and masked+CBAM enhanced feature maps\n",
    "for layer_name, data in feature_maps_data.items():\n",
    "    image_name = data['image_name']\n",
    "    original_map = data['original']\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    fig, axs = plt.subplots(1, 2 if 'masked' in data else 1, figsize=(16, 8))\n",
    "    \n",
    "    # Plot original feature map\n",
    "    if 'masked' in data:\n",
    "        ax1 = axs[0]\n",
    "    else:\n",
    "        ax1 = axs\n",
    "    \n",
    "    im1 = ax1.imshow(original_map[0, 0].cpu().detach().numpy(), cmap='viridis')\n",
    "    ax1.set_title(f\"Original Feature Map - {layer_name}\")\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Plot masked+CBAM feature map if available\n",
    "    if 'masked' in data:\n",
    "        masked_map = data['masked']\n",
    "        im2 = axs[1].imshow(masked_map[0, 0].cpu().detach().numpy(), cmap='viridis')\n",
    "        axs[1].set_title(f\"Masked+CBAM Feature Map - {layer_name}\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "    \n",
    "    plt.suptitle(f\"Feature Maps from {layer_name} (Image: {image_name})\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(test_path, f\"{image_name}_{layer_name}_cbam_comparison.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Always clear hooks when done\n",
    "feature_extractor.clear_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d3ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mariopasc/Python/Datasets/COMBINED/YOLO_MGA/detection/images/val/arcadetest_p66_v66_00066.png: 640x640 1 person, 4.2ms\n",
      "Speed: 0.9ms preprocess, 4.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MaskedFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A hook manager that extracts feature maps from specified layers\n",
    "    and applies corresponding masks with skip connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_layers: List[str], mask_folder: str, alpha: float = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor.\n",
    "\n",
    "        Args:\n",
    "            target_layers: List of layer names to extract features from\n",
    "            mask_folder: Path to the folder containing image masks\n",
    "            alpha: Weight for skip connection (0.0 to 1.0)\n",
    "                   feature_map = (1-alpha)*feature_map + alpha*(feature_map*mask)\n",
    "        \"\"\"\n",
    "        self.target_layers = target_layers\n",
    "        self.mask_folder = mask_folder\n",
    "        self.alpha = max(0.0, min(1.0, alpha))  # Clamp between 0 and 1\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Storage dictionaries\n",
    "        self.feature_maps: Dict[str, torch.Tensor] = {}\n",
    "        self.masked_feature_maps: Dict[str, torch.Tensor] = {}\n",
    "        self.current_image_path: Optional[str] = None\n",
    "        self.image_names: Dict[str, str] = {}\n",
    "        \n",
    "    def set_current_image(self, image_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the current image being processed.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "        \"\"\"\n",
    "        self.current_image_path = image_path\n",
    "        \n",
    "    def register_hooks(self, model: nn.Module) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Register hooks to extract feature maps from the model.\n",
    "\n",
    "        Args:\n",
    "            model: The YOLO model to attach hooks to\n",
    "\n",
    "        Returns:\n",
    "            The model with hooks attached\n",
    "        \"\"\"\n",
    "        # Clear existing hooks and storage\n",
    "        self.clear_hooks()\n",
    "        self.feature_maps = {}\n",
    "        self.masked_feature_maps = {}\n",
    "        self.image_names = {}\n",
    "\n",
    "        # Register hooks on target layers\n",
    "        if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
    "            for name, module in model.model.named_modules():\n",
    "                if name in self.target_layers:\n",
    "                    # Register hook to capture feature map\n",
    "                    hook = self._create_feature_hook(name)\n",
    "                    self.hooks.append(module.register_forward_hook(hook))\n",
    "                    \n",
    "        return model\n",
    "\n",
    "    def _create_feature_hook(self, layer_name: str):\n",
    "        \"\"\"\n",
    "        Create a hook function to store feature maps and apply masks.\n",
    "\n",
    "        Args:\n",
    "            layer_name: Name of the layer for reference\n",
    "\n",
    "        Returns:\n",
    "            Hook function to be registered\n",
    "        \"\"\"\n",
    "        def hook(module, input_feat, output):\n",
    "            # Store original feature map\n",
    "            self.feature_maps[layer_name] = output.clone()\n",
    "            \n",
    "            # Store the image name if available\n",
    "            if self.current_image_path:\n",
    "                image_name = Path(self.current_image_path).stem\n",
    "                self.image_names[layer_name] = image_name\n",
    "                \n",
    "                # Find and apply mask if available\n",
    "                mask_path = self._find_mask_path(image_name)\n",
    "                if mask_path:\n",
    "                    # Process the mask and apply it\n",
    "                    mask_tensor = self._process_mask(mask_path, output.shape[2:])\n",
    "                    if mask_tensor is not None:\n",
    "                        mask_tensor = mask_tensor.to(output.device)\n",
    "                        masked_output = self._apply_mask_with_skip(output, mask_tensor)\n",
    "                        self.masked_feature_maps[layer_name] = masked_output\n",
    "            \n",
    "            # Don't modify the output passing through the model\n",
    "            return output\n",
    "            \n",
    "        return hook\n",
    "    \n",
    "    def _find_mask_path(self, image_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find the corresponding mask file for an image.\n",
    "        \n",
    "        Args:\n",
    "            image_name: Base name of the image without extension\n",
    "            \n",
    "        Returns:\n",
    "            Path to the mask file if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Try the exact same filename with different possible extensions\n",
    "        for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "            mask_path = os.path.join(self.mask_folder, f\"{image_name}{ext}\")\n",
    "            if os.path.exists(mask_path):\n",
    "                return mask_path\n",
    "        \n",
    "        # If not found, look for any file starting with the image name\n",
    "        if os.path.exists(self.mask_folder):\n",
    "            for filename in os.listdir(self.mask_folder):\n",
    "                if filename.startswith(image_name):\n",
    "                    return os.path.join(self.mask_folder, filename)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _process_mask(self, mask_path: str, target_size: Tuple[int, int]) -> Optional[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load and process a mask to match the feature map dimensions.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to the mask file\n",
    "            target_size: Target size as (height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Processed mask tensor or None if processing failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load mask as grayscale image\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            \n",
    "            # Resize to match feature map dimensions\n",
    "            resized_mask = transforms.Resize(\n",
    "                target_size, interpolation=transforms.InterpolationMode.NEAREST\n",
    "            )(mask)\n",
    "            \n",
    "            # Convert to tensor [1, 1, H, W]\n",
    "            mask_tensor = transforms.ToTensor()(resized_mask).unsqueeze(0)\n",
    "            \n",
    "            return mask_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing mask {mask_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _apply_mask_with_skip(self, feature_map: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply mask to feature map with skip connection.\n",
    "        \n",
    "        Implements: feature_map = (1-alpha)*feature_map + alpha*(feature_map*mask)\n",
    "        \n",
    "        Args:\n",
    "            feature_map: Input feature map [B, C, H, W]\n",
    "            mask: Binary mask [1, 1, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            Modified feature map with same shape as input\n",
    "        \"\"\"\n",
    "        # Expand mask to match feature map channels\n",
    "        B, C, H, W = feature_map.shape\n",
    "        expanded_mask = mask.expand(B, C, H, W)\n",
    "        \n",
    "        # Apply mask with skip connection\n",
    "        masked_feature = feature_map * expanded_mask\n",
    "        output = (1 - self.alpha) * feature_map + self.alpha * masked_feature\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def clear_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks to prevent memory leaks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def get_feature_maps(self, include_masked: bool = True) -> Dict[str, Dict[str, Union[torch.Tensor, str]]]:\n",
    "        \"\"\"\n",
    "        Get the captured feature maps with their associated image names.\n",
    "        \n",
    "        Args:\n",
    "            include_masked: Whether to include masked versions of feature maps\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with layer names as keys and dict of data as values\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for layer_name, feature_map in self.feature_maps.items():\n",
    "            layer_data = {\n",
    "                'original': feature_map,\n",
    "                'image_name': self.image_names.get(layer_name)\n",
    "            }\n",
    "            \n",
    "            if include_masked and layer_name in self.masked_feature_maps:\n",
    "                layer_data['masked'] = self.masked_feature_maps[layer_name]\n",
    "                \n",
    "            result[layer_name] = layer_data\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def set_alpha(self, alpha: float) -> None:\n",
    "        \"\"\"\n",
    "        Set the weight for the skip connection.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Value between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        self.alpha = max(0.0, min(1.0, alpha))\n",
    "# Example usage in notebook cell\n",
    "\n",
    "from ICA_Detection.external.ultralytics.ultralytics import YOLO\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base = \"/home/mariopasc/Python/Datasets/COMBINED/YOLO_MGA\"\n",
    "\n",
    "# Image and mask paths\n",
    "image_path = os.path.join(base, \"detection\", \"images\", \"val\", \"arcadetest_p66_v66_00066.png\")\n",
    "mask_folder = os.path.join(base, \"masks\")\n",
    "test_path = \"/home/mariopasc/Python/Datasets/COMBINED/detection/testing\"\n",
    "\n",
    "# Initialize the feature extractor with target layers and mask folder\n",
    "target_layers = [\"model.15\", \"model.18\", \"model.21\"]\n",
    "feature_extractor = MaskedFeatureExtractor(\n",
    "    target_layers=target_layers,\n",
    "    mask_folder=mask_folder,\n",
    "    alpha=0.5  # Weight for mask application (adjust as needed)\n",
    ")\n",
    "\n",
    "# Load your YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Register hooks\n",
    "feature_extractor.register_hooks(model)\n",
    "\n",
    "# Set the current image path before inference\n",
    "feature_extractor.set_current_image(image_path)\n",
    "\n",
    "# Run inference with your model\n",
    "results = model(image_path)\n",
    "\n",
    "# Get the extracted feature maps with image names\n",
    "feature_maps_data = feature_extractor.get_feature_maps()\n",
    "\n",
    "# Visualize the original and masked feature maps\n",
    "for layer_name, data in feature_maps_data.items():\n",
    "    image_name = data['image_name']\n",
    "    original_map = data['original']\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    fig, axs = plt.subplots(1, 2 if 'masked' in data else 1, figsize=(16, 8))\n",
    "    \n",
    "    # Plot original feature map\n",
    "    if 'masked' in data:\n",
    "        ax1 = axs[0]\n",
    "    else:\n",
    "        ax1 = axs\n",
    "    \n",
    "    im1 = ax1.imshow(original_map[0, 0].cpu().detach().numpy(), cmap='viridis')\n",
    "    ax1.set_title(f\"Original Feature Map - {layer_name}\")\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Plot masked feature map if available\n",
    "    if 'masked' in data:\n",
    "        masked_map = data['masked']\n",
    "        im2 = axs[1].imshow(masked_map[0, 0].cpu().detach().numpy(), cmap='viridis')\n",
    "        axs[1].set_title(f\"Masked Feature Map - {layer_name}\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "    \n",
    "    plt.suptitle(f\"Feature Maps from {layer_name} (Image: {image_name})\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(test_path, f\"{image_name}_{layer_name}_comparison.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Always clear hooks when done\n",
    "feature_extractor.clear_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e639385",
   "metadata": {},
   "source": [
    "Here we have a simple feature extractor that returns the feature map and the name of the input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class SimpleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A simple hook manager that extracts feature maps from specified layers\n",
    "    of a YOLOv8 model without modifying them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_layers: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor.\n",
    "\n",
    "        Args:\n",
    "            target_layers: List of layer names to extract features from\n",
    "        \"\"\"\n",
    "        self.target_layers = target_layers\n",
    "        self.hooks = []\n",
    "        self.feature_maps: Dict[str, torch.Tensor] = {}\n",
    "        self.current_image_path: Optional[str] = None\n",
    "        self.image_names: Dict[str, str] = {}\n",
    "        \n",
    "    def set_current_image(self, image_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the current image being processed.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "        \"\"\"\n",
    "        self.current_image_path = image_path\n",
    "        \n",
    "    def register_hooks(self, model: nn.Module) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Register hooks to extract feature maps from the model.\n",
    "\n",
    "        Args:\n",
    "            model: The YOLO model to attach hooks to\n",
    "\n",
    "        Returns:\n",
    "            The model with hooks attached\n",
    "        \"\"\"\n",
    "        # Clear existing hooks and feature maps\n",
    "        self.clear_hooks()\n",
    "        self.feature_maps = {}\n",
    "        self.image_names = {}\n",
    "\n",
    "        # Register hooks on target layers\n",
    "        if hasattr(model, 'model') and isinstance(model.model, nn.Module):\n",
    "            for name, module in model.model.named_modules():\n",
    "                if name in self.target_layers:\n",
    "                    # Register hook to capture feature map\n",
    "                    hook = self._create_feature_hook(name)\n",
    "                    self.hooks.append(module.register_forward_hook(hook))\n",
    "                    \n",
    "        return model\n",
    "\n",
    "    def _create_feature_hook(self, layer_name: str):\n",
    "        \"\"\"\n",
    "        Create a hook function to store feature maps.\n",
    "\n",
    "        Args:\n",
    "            layer_name: Name of the layer for reference\n",
    "\n",
    "        Returns:\n",
    "            Hook function to be registered\n",
    "        \"\"\"\n",
    "        def hook(module, input_feat, output):\n",
    "            self.feature_maps[layer_name] = output.clone()\n",
    "            \n",
    "            # Store the image name if available\n",
    "            if self.current_image_path:\n",
    "                image_name = Path(self.current_image_path).stem\n",
    "                self.image_names[layer_name] = image_name\n",
    "                \n",
    "            return output\n",
    "            \n",
    "        return hook\n",
    "\n",
    "    def clear_hooks(self) -> None:\n",
    "        \"\"\"Remove all registered hooks to prevent memory leaks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def get_feature_maps(self) -> Dict[str, Tuple[torch.Tensor, Optional[str]]]:\n",
    "        \"\"\"\n",
    "        Get the captured feature maps with their associated image names.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with layer names as keys and tuples of (feature_map, image_name) as values\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for layer_name, feature_map in self.feature_maps.items():\n",
    "            image_name = self.image_names.get(layer_name)\n",
    "            result[layer_name] = (feature_map, image_name)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def get_image_names(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get the image names associated with each layer's feature maps.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with layer names as keys and image names as values\n",
    "        \"\"\"\n",
    "        return self.image_names\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ica_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
